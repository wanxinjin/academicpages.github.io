---
title: 'Pontryagin Differentiable Programming'
date: 2020-04-7
permalink: /posts/pdp
tags:
  - PDP
  - Inverse optimization
  - System identification
  - Motion planning
---

# <center> Abstract <center>
This paper develops a Pontryagin differentiable programming (PDP) methodology, which establishes a unified framework to solve a broad class of learning and control tasks. The PDP methodology distinguishes from existing methods by two novel techniques: first, we  differentiate the Pontryagin's Maximum Principle,  and this allows us to obtain analytical gradient of a trajectory with respect to a tunable parameter of a system, thus enabling end-to-end learning of system dynamics model, policy, or/and control objective function; and second, we propose an auxiliary control system in backward pass of the PDP framework, and show that the output of the auxiliary control system is exactly the gradient of the system trajectory with respect to the parameter, which can be iteratively obtained using control tools. We investigate three learning modes of the PDP: inverse reinforcement learning,  system identification, and  control/planning, respectively. We  demonstrate the capability of the PDP in each learning mode using various high-dimensional systems, including multilink robot arm,  6-DoF maneuvering UAV, and 6-DoF rocket powered landing.
<br />
<hr>
<br />


# <center> Pontryagin Differentiable Programming Framework <center>
<p style="margin-bottom:1cm; margin-left: 0.5cm"> </p>


## The Base Problem
$$
\begin{equation}\label{loss}
\min_{\boldsymbol{\theta}}L(\boldsymbol{\xi}_{\boldsymbol{\theta}},\boldsymbol{\theta})
\end{equation}\quad \text{subject to}
$$

$$\boldsymbol{\xi}_{\boldsymbol{\theta}} \text{ is the trajectory generated by a parameterized dynamical system } \boldsymbol{\Sigma}(\boldsymbol{\theta})\label{system}$$


* $$\boldsymbol{\Sigma}(\boldsymbol{\theta})$$: a parameterized dynamical system with $$\boldsymbol{\theta}$$ denoting a tunable parameter, which could be in system dynamics model, control policy, or/and control objective function, depending on different learning and control applications, as will be discussed below.

* $$L(\boldsymbol{\xi}_{\boldsymbol{\theta}},\boldsymbol{\theta})$$: a specific scalar-valued loss function that evaluates the system trajectory $$\boldsymbol{\xi}_{\boldsymbol{\theta}}$$. This loss function is set as per specific tasks, as will be discussed below.






## PDP Learning and Control Framework
<p style="margin-bottom:1cm; margin-left: 0.5cm"> </p>
<center>
<img src="/images/overview2.jpg" width="700" align="center" />
<p style="margin-top:0.5cm;"> Architecture of the PDP learning and control framework (each learning update) </p>
</center>


### End-to-End Parameter Tuning:

$$\boldsymbol{\theta}_{k+1}=\boldsymbol{\theta}_{k}-\eta_{k}\frac{d L}{d \boldsymbol{\theta}}\Bigr|_{\boldsymbol{\theta}_k}$$


* <span style="color:#3883c0">**_Forward Pass_**</span>:

	* $$\boldsymbol{\Sigma}(\boldsymbol{\theta})$$: the parameterized system with current guess of $$\boldsymbol{\theta}$$, which could be in system dynamics, policy, or/and control objective function, depending on different applications, as will be discussed below.

	* $$\boldsymbol{\xi}_{\boldsymbol{\theta}}$$: the system trajectory generated by the current parameterized system $$\boldsymbol{\Sigma}(\boldsymbol{\theta})$$.

	* $$L(\boldsymbol{\xi}_{\boldsymbol{\theta}},\boldsymbol{\theta})$$: the specific scalar-valued loss function that evaluates the system trajectory $$\boldsymbol{\xi}_{\boldsymbol{\theta}}$$. This loss function is set as per specific tasks, as will be discussed below.


* <span style="color:#e58c00">**_Backward Pass_** </span>:
	* $$\boldsymbol{\overline{\Sigma}}(\boldsymbol{\xi}_{\boldsymbol{\theta}})$$: the auxiliary control system, which takes as input the system trajectory $$\boldsymbol{\xi}_{\boldsymbol{\theta}}$$ and outputs the gradient of the trajectory with respect to the tunable parameter, $$\frac{\partial \boldsymbol{\xi}_{\boldsymbol{\theta}}}{\partial \boldsymbol{\theta}}$$.

	* $$\frac{\partial L}{\partial \boldsymbol{\xi}}$$ and $$\frac{\partial  L}{\partial \boldsymbol{\theta}}$$: the partial gradients of the loss with respect to the system trajectory $$\boldsymbol{\xi}_{\boldsymbol{\theta}}$$ and the tunable parameter $$\boldsymbol{\theta}$$, respectively.

	*  $$\frac{d L}{d \boldsymbol{\theta}}=\frac{\partial L}{\partial \boldsymbol{\xi}}\frac{\partial  \boldsymbol{\xi}_{\boldsymbol{\theta}}}{\partial \boldsymbol{\theta}}+\frac{\partial L}{\partial \boldsymbol{\theta}}$$: use the chain rule to solve the total gradient  $\boldsymbol{\theta}$.





<br />
<hr>
<br />








# <center> PDP for Inverse Reinforcement Learning Mode <center>
<p style="margin-bottom:1cm; margin-left: 0.5cm"> </p>
Inverse reinforcement learning, a.k.a., inverse optimal control, focuses on learning an objective function of an optimal contorl system (sometime also with learning system dynamics model) such that its trajectory can finally match or emulate the observed demonstrations.


##  Formulation under PDP framework
We use $$\boldsymbol{\Sigma}(\boldsymbol{\theta})$$ to denote a parameterized optimal control system, with a tunable $$\boldsymbol{\theta}\in\mathbb{R}^r$$  in both  dynamics and control objective function:

$$
\hspace{-50pt}
\boldsymbol{\Sigma}(\boldsymbol{\theta}):\qquad
\boxed{
\begin{aligned}\qquad
\text{dynamics:}&\qquad \boldsymbol{x}_{t{+}1}=\boldsymbol{f}(\boldsymbol{x}_{t},\boldsymbol{u}_{t}, {\boldsymbol{\theta}}) \quad  \text{with}  \,\, \text{given}\,\,\boldsymbol{x}_0,\\
\text{objective function:}&\qquad J(\boldsymbol{\theta})=\sum\nolimits_{t=0}^{T{-}1}c_t(\boldsymbol{x}_t,\boldsymbol{u}_t, {\boldsymbol{\theta}})+h(\boldsymbol{x}_T,{\boldsymbol{\theta}}).
\end{aligned}
}
$$

where $$t=0,1,\cdots, T$$ is the time step with $$T$$ being the time horizon. For a choice of $$\boldsymbol{\theta}$$, the above system $$\boldsymbol{\Sigma}(\boldsymbol{\theta})$$ produces an optimal trajectory of state-inputs
$$\boldsymbol{\xi}_{\boldsymbol{\theta}}=\{\boldsymbol{x}_{0:T}^{\boldsymbol{\theta}},\boldsymbol{u}_{0:T-1}^{\boldsymbol{\theta}}\}$$
which optimizes the control objective $$J(\boldsymbol{\theta})$$ and constraint to dynamics $$\boldsymbol{f}$$.


Suppose that we are given a set of  demonstrations $$\boldsymbol{\xi}^d=\{\boldsymbol{x}_{0:T}^d,\boldsymbol{u}^d_{0:T-1}\}$$. We want to train the optimal control system  $$\boldsymbol{\Sigma}(\boldsymbol{\theta})$$ such that its trajectory $$\boldsymbol{\xi}_{\boldsymbol{\theta}}$$ emulate the demonstration $$\boldsymbol{\xi}^d$$; and thus we specify the loss function in (\ref{loss}) as

$$L(\boldsymbol{\xi}_{\boldsymbol{\theta}},\boldsymbol{\theta})=\text{E}_{\boldsymbol{\xi}^d}[l(\boldsymbol{\xi}_{\boldsymbol{\theta}}, \boldsymbol{\xi}^d)],$$

where  $$l$$ is a scalar function that penalizes the inconsistency of $$\boldsymbol{\xi}_{\boldsymbol{\theta}}$$  from the demonstration $$\boldsymbol{\xi}^d$$, for example, 
$$l(\boldsymbol{\xi}_{\boldsymbol{\theta}}, \boldsymbol{\xi}^d)=||\boldsymbol{\xi}_{\boldsymbol{\theta}}-\boldsymbol{\xi}^d||^2$$. The problem in (\ref{loss}) and (\ref{system}) formulates the inverse reinforcement learning or inverse optimal control problem.

##  Simulation demos 
<center>
<img src="/images/uav_imitation.gif" alt="drawing" width="600" align="center"/>
<p style="margin-top:0.5cm;"> Imitation learning for 6 DoF UAV maneuvering <a href="https://www.youtube.com/watch?v=awVNiCIJCfs" >[<img src="/images/youtube.png" width="25px" height="25px">]</a> </p>
</center>

<p style="margin-bottom:1cm; margin-left: 0.5cm"> </p>

<center>
<img src="/images/rocket_imitation.gif" alt="drawing" width="600" align="center"/>
<p style="margin-top:0.5cm;"> Imitation learning for 6 DoF rocket powered landing <a href="https://youtu.be/4RxDLxUcMp4">[<img src="/images/youtube.png" width="25px" height="25px">]</a> </p>
</center>
<br />
<hr>
<br />









# <center> PDP for Control and Planning <center>
<p style="margin-bottom:1cm; margin-left: 0.5cm"> </p>
We show how to apply the PDP framework to solve control or planning problems, whose goal is to find a control policy or trajectory such that a given cost function is minimized.


##  Formulation under PDP framework
We use $\boldsymbol{\Sigma}(\boldsymbol{\theta})$ to formulate a feedback controlled system:

$$
\boldsymbol{\Sigma}(\boldsymbol{\theta}):\qquad
\boxed{
\begin{aligned}
	\text{dynamics:} &\quad \boldsymbol{x}_{t+1}=\boldsymbol{f}(\boldsymbol{x}_{t},\boldsymbol{u}_{t}) \quad  \text{with}  \quad\,\,\boldsymbol{x}_0, \\
	\text{control policy:} &\quad \boldsymbol{u}_t=\boldsymbol{u}(t,\boldsymbol{x}_t,\boldsymbol{\theta}).
	\end{aligned}
}$$

where $$\boldsymbol{u}_t=\boldsymbol{u}(t,\boldsymbol{x}_t,\boldsymbol{\theta})$$ is the parameterized control policy and $\boldsymbol{f}$ is the dynamics model. For a fixed choice of $$\boldsymbol{\theta}$$ for the control policy, the above controlled system will produce a trajectory   $$\boldsymbol{\xi}_{\boldsymbol{\theta}}=\{\boldsymbol{x}^{\boldsymbol{\theta}}_{0:T},\boldsymbol{u}_{0:T-1}^{\boldsymbol{\theta}}\}$$.


To achieve the optimal control performance, we set the loss function $$L(\boldsymbol{\xi}_{\boldsymbol{\theta}},\boldsymbol{\theta})$$ in (\ref{loss}) as 

$$\label{lossmodeplan}
L(\boldsymbol{\xi}_{\boldsymbol{\theta}},\boldsymbol{\theta})=\sum\nolimits_{t=0}^{T{-}1}l(\boldsymbol{x}^{\boldsymbol{\theta}}_t,\boldsymbol{u}^{\boldsymbol{\theta}}_t) +l_f(\boldsymbol{x}^{\boldsymbol{\theta}}_{T})
$$

where $l$ and $l_f$ are the stage and final  cost functions, respectively. Then, the problem in (\ref{loss}) and (\ref{system}) formulates the optimal control or planning problem.

##  Simulation demos 
<center>
<img src="/images/uav_control.gif" alt="drawing" width="600" align="center"/>
<p style="margin-top:0.5cm;"> 6 DoF UAV maneuvering control <a href="https://youtu.be/KTw6TAigfPY">[<img src="/images/youtube.png" width="25px" height="25px">]</a> </p>
</center>
<p style="margin-bottom:1cm; margin-left: 0.5cm"> </p>
<center>
<img src="/images/rocket_control.gif" alt="drawing" width="600" align="center"/>
<p style="margin-top:0.5cm;"> 6 DoF rocket powered landing control <a href="https://youtu.be/5Jsu772Sqcg">[<img src="/images/youtube.png" width="25px" height="25px">]</a> </p>
</center>
<br />
<hr>
<br />



# <center> PDP for System Identification <center>
<p style="margin-bottom:1cm; margin-left: 0.5cm"> </p>
We show how to apply the PDP framework to solve system identification, whose goal is to identify a dynamics governing equation from the observation of input-state data of a (physical) unknown system.

##  Formulation under PDP framework
Suppose that we are given  input-output data $$\boldsymbol{\xi}^o=\{\boldsymbol{x}_{0:T}^o, \boldsymbol{u}_{0:T-1}\}$$ recorded from, say, a physical system, and we wish to identify the system's dynamics equation.  To that end,  we use the following parameterized system 
$$\boldsymbol{\Sigma}(\boldsymbol{\theta})$$:

$$
\hspace{-20pt}
\boldsymbol{\Sigma}(\boldsymbol{\theta}):\qquad
\boxed{
\text{dynamics:} \quad
	\boldsymbol{x}_{t+1}=\boldsymbol{f}(\boldsymbol{x}_{t},\boldsymbol{u}_{t}, {\boldsymbol{\theta}}) \qquad  \text{with }  \boldsymbol{x}_0 \ \text{and external } \,\boldsymbol{u}_{0:T{-}1}.
}$$

Now, the above $$\boldsymbol{\Sigma}(\boldsymbol{\theta})$$  will produce a trajectory  $$\boldsymbol{\xi}_{\boldsymbol{\theta}}=\{\boldsymbol{x}^{\boldsymbol{\theta}}_{0:T},\boldsymbol{u}_{0:T-1}\}.$$ 

For system identification, we define the loss function  as

$$\label{lossid}
L(\boldsymbol{\xi}_{\boldsymbol{\theta}},\boldsymbol{\theta})=\text{E}_{\boldsymbol{\xi}^o}[l(\boldsymbol{\xi}_{\boldsymbol{\theta}}, \boldsymbol{\xi}^o)],
$$

where $l$ is a differentiable scalar-valued penalty function to quantify the reproduction error between the recorded data $$\boldsymbol{\xi}^{\boldsymbol{o}}=\{\boldsymbol{x}_{0:T}^{\boldsymbol{o}}, \boldsymbol{u}_{0:T-1} \}$$  and simulated data $$\boldsymbol{\xi}_{\boldsymbol{\theta}}=\{\boldsymbol{x}_{0:T}^{\boldsymbol{\theta}}, \boldsymbol{u}_{0:T-1} \}$$ under the same supplied inputs $$\boldsymbol{u}_{0:T-1}$$. The problem in (\ref{loss}) and (\ref{system}) formulates the system identification problem.

##  Simulation demos 
<center>
<img src="/images/cartpole_ID.gif" alt="drawing" width="600" align="center"/>
<p style="margin-top:0.5cm;"> 6 DoF UAV maneuvering control <a href="https://youtu.be/PAyBZjDD6OY">[<img src="/images/youtube.png" width="25px" height="25px">]</a> </p>
</center>
<br />
<hr>
<br />
=======
>>>>>>> parent of 24fd244... first version
=======
>>>>>>> parent of 24fd244... first version
=======

Key proposed techniques in PDP
======


=======
>>>>>>> parent of 24fd244... first version
Algorithms
======


Simulation
======

<<<<<<< HEAD
>>>>>>> parent of 24fd244... first version
=======
>>>>>>> parent of 24fd244... first version



