---
title: 'Learning from Incremental Directional Corrections'
date: 2020-10-1
permalink: /posts/lfdc
tags:
  - human-robot interaction
  - learning from corrections
---


<p style="margin-bottom:1cm; margin-left: 0.5cm"> </p>

# <center> Abstract <center>
 This paper proposes a technique which enables a robot to learn a control
objective function incrementally from human user's corrections. The human's
corrections can be as simple as directional corrections---corrections that
indicate the direction of a control change without indicating its
magnitude---applied at some time instances during the robot's motion. We only
assume that each of the human's corrections, regardless of its magnitude,
points in a direction that improves the robot's current motion relative to an
implicit objective function. The proposed method uses the direction of a
correction to update the estimate of the objective function based on a cutting
plane technique. We establish the theoretical results to show that this process
of incremental correction and update guarantees convergence of the learned
objective function to the implicit one. The method is validated by both
simulations and two human-robot games, where human players teach a 2-link robot
arm and a 6-DoF quadrotor system for motion planning in environments with
obstacles.

-----

<br />
<br />



# <center> Problem Formulation <center>

Consider a robot with the following dynamics:

$$\label{equ_dynamics}
\boldsymbol{x}_{t+1}=\boldsymbol{f}(\boldsymbol{x}_{t},\boldsymbol{u}_{t}), \quad \text{with} \quad\boldsymbol{x}_0,
$$

where $$\boldsymbol{x}_t\in\mathbb{R}^{n}$$ is the robot state, $$\boldsymbol{u}_t\in\mathbb{R}^{m}$$ is the  control input,   $$\boldsymbol{f}:\mathbb{R}^n\times\mathbb{R}^m\mapsto\mathbb{R}^n$$ is  differentiable, and $$t=1,2,\cdots$$ is the time step. As commonly used by objective learning methods, we suppose that the cost function obeys the following parameterized form

$$\label{equ_objective}
J(\boldsymbol{u}_{0:T},\boldsymbol{\theta})=\sum\nolimits_{t=0}^{T}\boldsymbol{\theta}^\prime\boldsymbol{\phi}(\boldsymbol{x}_{t},\boldsymbol{u}_{t})+h(\boldsymbol{x}_{T+1}),
$$

where $$\boldsymbol{\phi}:\mathbb{R}^n\times \mathbb{R}^m\mapsto\mathbb{R}^{r}$$  is a vector of the **specified** features (or basis functions) for the  running cost;  $$\boldsymbol{\theta}\in\mathbb{R}^{r}$$ is a  vector of  weights, which are  **tunable**;  and  $$h(\boldsymbol{x}_{T+1})$$ is the  final cost that penalizes the final  state $$\boldsymbol{x}_{T+1}$$. For a given choice of $$\boldsymbol{\theta}$$, the  robot chooses a sequence of  inputs $$\boldsymbol{u}_{0:T}$$ over the time horizon $$T$$ by optimizing (\ref{equ_objective})  subject to (\ref{equ_dynamics}),  producing a trajectory

$$\label{equ_trajectory}
\boldsymbol{\xi}_{\boldsymbol{\theta}}=\left\{\boldsymbol{x}_{0:T\text{+}1}^{\boldsymbol{\theta}}, \boldsymbol{u}_{0:T}^{\boldsymbol{\theta}}\right\}.
$$

For notation simplicity,  we  occasionally write the cost function (\ref{equ_objective}) as   $$J(\boldsymbol{\theta})$$.



### _Why Human Corrections and How?_

For a specific task,
suppose that a human's expectation of the robot's  trajectory corresponds to an **implicit** cost function   $$J(\boldsymbol{\theta}^*)$$ in the same form of (\ref{equ_objective}) with  $$\boldsymbol{\theta}^*$$. Here, we    call  $$\boldsymbol{\theta}^*$$  the **expected weight vector**. In general cases,  a  human  user
may neither explicitly write down the value of $$\boldsymbol{\theta}^*$$ nor  demonstrate the corresponding optimal trajectory $$\boldsymbol{\xi}_{\boldsymbol{\theta}^*}$$ to the robot,  but
the human user  can tell  whether the robot's  current trajectory is **satisfactory** or not. A trajectory of the robot is satisfactory if it minimizes  $$J(\boldsymbol{\theta}^*)$$; otherwise, it is not satisfactory.   In order for the robot to  achieve  $$J(\boldsymbol{\theta}^*)$$ (and thus  generates a satisfactory trajectory),  the human user is only able to  make  corrections  to the robot during its motion,
based on which  the robot updates its  guess of $$\boldsymbol{\theta}$$ towards  $$\boldsymbol{\theta}^*$$.


The process for a robot to learn from human's corrections in this paper is iterative. Each iteration basically includes three steps: planning, correction and update. Let $k=1,2, 3,\cdots$, denote the iteration index  and let  $$\boldsymbol{\theta}_k$$ denote the robot's weight vector guess at iteration $$k$$.  At  $$k=1$$, the robot is initialized with an arbitrary weight vector guess $$\boldsymbol{\theta}_1$$.  At iteration $$k=1,2,3,\cdots$$, the robot first performs trajectory **planning**, i.e. achieves  $$\boldsymbol{\xi}_{\boldsymbol{\theta}_k}$$  by
minimizing
the cost function $$J(\boldsymbol{\theta}_k)$$ in (\ref{equ_objective}) subject to  its dynamics   (\ref{equ_dynamics}).  During robot's execution of   $$\boldsymbol{\xi}_{\boldsymbol{\theta}_k}$$, the human user gives a **correction** denoted by $$\boldsymbol{a}_{t_k}\in\mathbb{R}^m$$ to the robot in its input space. Here, $${t_k}\in\{0,1,\cdots,T\}$$,  called correction time,  indicates at which time step within the horizon $$T$$  the  correction is made.  After  receiving  $$\boldsymbol{a}_{t_k}$$, the robot then performs **update**, i.e. change its  guess $$\boldsymbol{\theta}_k$$  to $$\boldsymbol{\theta}_{k+1}$$ according to an update rule to be developed later.


_**Remark:**_ *We assume  that  human user's corrections $$\boldsymbol{a}_{t_k}\in\mathbb{R}^m$$  are  in the robot's input space,  which means that $$\boldsymbol{a}_{t_k}$$ can be directly added to the robot's  input $$\boldsymbol{u}_{t_k}$$. This can be  satisfied in some  cases such as autonomous driving, where a  user directly manipulates  the steering angle of a  vehicle. For other cases where the    corrections are not readily in the robot's  input space, this requirement could be fulfilled through certain human-robot interfaces, which  translate the  correction signals into the   input space. Then, $$\boldsymbol{a}_{t_k}$$ denotes the translated correction. The reason why we do not consider the corrections in the robot's state space is that 1) the input corrections may be easier in implementation, and 2) the  corrections  in the state space  can be infeasible for  some under-actuated robot systems.*



### _Key Assumption on Human's Directional Corrections_
In the above process, the human's  correction $$\boldsymbol{a}_{t_k}$$   is generally not  once-and-for-all, but rather an **incremental** improvement of the robot's current motion  $$\boldsymbol{\xi}_{\boldsymbol{\theta}_k}$$ under the  implicit  $$J(\boldsymbol{\theta}^*)$$.
Each human's  correction $$\boldsymbol{a}_{t_k}$$ is assumed to satisfy the following condition:

$$
\label{equ_assumption}
\left\langle -\nabla J(\boldsymbol{u}_{0:T}^{\boldsymbol{\theta}_k},\boldsymbol{\theta}^*),\,\,\boldsymbol{\bar{a}}_k \right\rangle>0, \quad  k=1,2,3,\cdots.
$$

Here
$$
\boldsymbol{\bar{a}}_{k}= \begin{bmatrix}
\boldsymbol{0}^\prime & \cdots & \boldsymbol{a}_{t_k}^\prime & \cdots,\boldsymbol{0}^\prime
\end{bmatrix}^\prime\in\mathbb{R}^{m{(T+1)}},
$$
with   $$\boldsymbol{a}_{t_k}$$ being the  $$t_k$$-th entry and   $$\boldsymbol{0}\in\mathbb{R}^m$$ else; $$\left\langle \cdot,\cdot\right\rangle$$ is the dot product; and $$-\nabla J(\boldsymbol{u}_{0:T}^{\boldsymbol{\theta}_k},\boldsymbol{\theta}^*)$$ is the gradient-descent of $$J(\boldsymbol{\theta}^*)$$ with respect to   $$\boldsymbol{u}_{0:T}$$ evaluated at robot's current   $$\boldsymbol{\xi}_{\boldsymbol{\theta}_k}=\{\boldsymbol{x}_{0:T\text{+}1}^{\boldsymbol{\theta}_k}, \boldsymbol{u}_{0:T}^{\boldsymbol{\theta}_k}\}$$. Note that the assumption in (\ref{equ_assumption}) does not require a specific value to the magnitude of $$\boldsymbol{a}_{t_k}$$ but requires its direction roughly around the gradient-descent direction of $$J(\boldsymbol{\theta}^*)$$. Such correction aims to guide the robot's trajectory $$\boldsymbol{\xi}_{\boldsymbol{\theta}_k}$$ towards reducing its cost under  $$J(\boldsymbol{\theta}^*)$$ unless the trajectory is satisfactory. Thus, we call $$\boldsymbol{a}_{t_k}$$ satisfying (\ref{equ_assumption}) the **incremental directional correction**.








<p style="margin-bottom:1cm; margin-left: 0.5cm"> </p>
<center>
<img src="/images/lfc_correction.png" width="250" align="center" />
  <p style="margin-top:0.5cm;font-size:15px">  Human's directional correction $\boldsymbol{a}_{t_k}$. The orange region is all feasible directional corrections that satisfy (\ref{equ_assumption}).  </p>
</center>

_**Remark:**_ *As shown in the below figure, the feasible directional corrections under (\ref{equ_assumption}) always account for the half of the entire input space.    The human  can choose any correction as long as its direction falls in the half space with  negative gradient of $$J(\boldsymbol{\theta}^*)$$. Thus, (\ref{equ_assumption}) is more likely to be satisfied especially for non-expert  users.*






### _The problem of Interest_
The problem of interest in this paper is  to develop a rule to update the robot's weight vector guess   $$\boldsymbol{\theta}_k$$ to $$\boldsymbol{\theta}_{k+1}$$ such that  $$\boldsymbol{\theta}_k$$ converges to $$\boldsymbol{\theta}^*$$ as $$k=1,2,3,\cdots$$,  with the  human's  directional corrections $$\boldsymbol{a}_{t_k}$$ under the assumption (\ref{equ_assumption}).

-----

<br />
<br />


# <center> Main Algorithm and Analysis <center>


<center>Please read the paper for the proposed method and its convergence analysis.</center>
-----

<p style="margin-bottom:2cm; margin-left: 0.5cm"> </p>



<br />
<br />


# <center> Experiments: Two Human-Robot Games <center>


 We  develop  two  human-robot computer games (environments), where a real human player  online teaches a  robot motion planning via directional corrections. These  games are used to validate  effectiveness of the proposed approach in practice.  The games are developed on  two robot systems: a two-link robot arm and  6-DoF maneuvering quadrotor, respectively. For each environment, a human player visually inspects robot's motion on a computer screen, meanwhile  providing interactive directional corrections through a keyboard.   The goal of each game  is to train a robot to learn a expected control objective function such that the robot successfully moves around obstacles  and reaches a target  position.

<br />



<table style=" border: none;">
<tr style="border: none;text-align:center;">
<td style="border: none; text-align:center;" > 
  <img src="/images/fig_robot_iteration0.png" style="width: 250px;"/>   
  <p style="margin-top:0.5cm;font-size:15px"> The robot arm game. The goal is to let a human player  teach the robot arm  to learn a valid cost function (i.e., the expected weight vector $\boldsymbol{\theta}^*$) by applying incremental directional corrections, such that it successfully moves from the initial condition (current pose)  to  the target position (upward pose) while avoiding the obstacle.  
   </p>
</td>
<td style="border: none; text-align:center;"> 
  <img src="/images/fig_uav_iteration0.png"  alt="Drawing" style="width: 330px;"/> 
<p style="margin-top:0.5cm;font-size:15px"> The 6-DoF quadrotor  game. The goal of this game is to let a human player to  teach a 6-DoF quadrotor system to learn a valid control cost function (i.e., the expected weight vector $\boldsymbol{\theta}^*$) by providing  directional corrections, such that it can successfully fly from the initial position (in bottom left),  pass through a  gate (colored in brown), and finally land on the specified target  (in upper right). </p>
</td>
</tr>
</table>




We have publicly released all codes together with the game environments for the readers to have  hands-on experience with.  Please download the games at <https://github.com/wanxinjin/Learning-from-Directional-Corrections>.

