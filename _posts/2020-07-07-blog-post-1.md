---
title: 'Learning from Sparse Demonstrations'
date: 2020-08-1
permalink: /posts/lfsd
tags:
  - inverse reinforcement learning
  - inverse optimal control
  - learning from demonstrations
---


<p style="margin-bottom:1cm; margin-left: 0.5cm"> </p>

# <center> Abstract <center>
This paper proposes an approach which enables a robot to learn  an objective function from sparse demonstrations of an expert. The demonstrations are given by a small number of sparse waypoints; the waypoints are desired outputs of the robot's trajectory at certain time instances, sparsely located within a demonstration time horizon. The duration of the expert's demonstration may be different from the actual duration of the robot's execution. The proposed method enables to jointly  learn an objective function and a time-warping function such that the robot's reproduced trajectory has  minimal distance to the sparse demonstration waypoints. Unlike existing inverse reinforcement learning techniques, the proposed approach uses the differential Pontryagin's maximum principle, which  allows    direct minimization of the distance between  the robot's trajectory and the sparse  demonstration waypoints  and  enables  simultaneous learning of an objective function and  a time-warping function. We demonstrate the  effectiveness of the proposed approach in various simulated scenarios. We apply the method to learn   motion planning/control  of a 6-DoF maneuvering unmanned aerial vehicle (UAV)  and a robot arm in environments with obstacles. The results show that a robot is able to learn a valid  objective function to avoid obstacles with few demonstrated waypoints. 
<br />
<br />



## Problem Statement


Consider a robot with the following continuous dynamics:
\begin{equation}\label{dyn}
\boldsymbol{\dot{x}}(t)=\boldsymbol{f}(\boldsymbol{x}(t),\boldsymbol{u}(t)) \quad \text{with} \quad \boldsymbol{x}(0),
\end{equation}
where $\boldsymbol{x}(t)\in\mathbb{R}^n$ is the robot state; $\boldsymbol{u}(t)\in\mathbb{R}^m$ is the control input;  vector function $\boldsymbol{f}:\mathbb{R}^n\times\mathbb{R}^m \mapsto \mathbb{R}^n$ is assumed to be twice-differentiable,  and $t\in[0,\infty)$ is time. Suppose  the robot motion over a time horizon $t_f>0$ is controlled by optimizing the following parameterized objective function:
\begin{equation}\label{costfun}
J(\boldsymbol{p})=\int_{0}^{t_f}c(\boldsymbol{x}(t),\boldsymbol{u}(t),{\boldsymbol{p}})dt+h(\boldsymbol{x}(t_f),{\boldsymbol{p}}),
\end{equation}
where  $c(\boldsymbol{x},\boldsymbol{u},{\boldsymbol{p}})$ and $h{(\boldsymbol{x},\boldsymbol{p}})$ are the running  and final  costs, respectively, both of which are  assumed twice-differentiable; and $\boldsymbol{p}\in\mathbb{R}^{r}$ is a tunable parameter vector. For a fixed choice of $\boldsymbol{p}$, the robot  produces a trajectory of states and inputs

$$
\boldsymbol{\xi}_{\boldsymbol{p}}=\{\boldsymbol{\xi}_{\boldsymbol{p}}(t)\,| \,0\leq t\leq t_f\} \quad \text{with}\quad \boldsymbol{\xi}_{\boldsymbol{p}}(t)=\{\boldsymbol{x}_{\boldsymbol{p}}(t),\boldsymbol{u}_{\boldsymbol{p}}(t)\},
$$

which optimizes the objective function (\ref{costfun}). Here the subscript in $\boldsymbol{\xi}_{\boldsymbol{p}}$ indicates that the  trajectory  implicitly depends on $\boldsymbol{p}$.




The goal of learning from demonstrations is to estimate the objective function parameter $\boldsymbol{p}$ based on the observed demonstrations of an expert (usually a human operator). 
Here, we suppose that an expert provides demonstrations through a known output function
\begin{equation}\label{interface}
\boldsymbol{y}=\boldsymbol{g}(\boldsymbol{x},\boldsymbol{u}),
\end{equation}
where  $\boldsymbol{g}:\mathbb{R}^n\times\mathbb{R}^m\rightarrow\mathbb{R}^o$ defines a map  from the robot's state and input to an output   $\boldsymbol{y}\in \mathbb{R}^{o}$. The~expert's  demonstrations include  (i) an <em>expected</em> time horizon $T$, and (ii) a number of $N$  waypoints, each of which is a desired  output for the robot  to reach at an <em>expected</em> time instance, denoted as

$$\label{corrections}
\mathcal{D}=\{\boldsymbol{y}^*(\tau_i)\,|\,
\tau_i\in[0,T],\, i=1,2, \cdots, N
\}.
$$

Here, $$\boldsymbol{y}^*(\tau_i)$$ is the $i$th  waypoint demonstrated by the expert, and ${\tau_i}$ is the expected time instance at which the expert wants the robot to reach the waypoint $$\boldsymbol{y}^*(\tau_i)$$. As the expert can freely provide the number of $$N$$  waypoints  and choose the positions of   expected time instances $$\tau_i$$ relative to the expected horizon $T$, we refer to  $$\mathcal{D}$$ as ***sparse demonstrations***. As will be shown later in simulations, $N$ here can be small.




Note that both the expected time horizon $T$ and the expected time instances $\tau_i$ are in the  time axis of the expert's demonstrations. This demonstration time axis may not be identical to the  actual time axis of  execution of the robot; in other words, the given times $T$ and $\tau_i$ may not be achievable by the robot. For example, when the robot is actuated by a weak servo motor, its motion inherently cannot meet the time step $\tau_i$ required  by a human demonstrator. To accommodate the  misalignment of duration between the robot and expert's demonstrations, we introduce a   ***time warping function***
\begin{equation}\label{warping_fun}
t=w(\tau),
\end{equation}
which defines  a map from the expert's demonstration time axis $\tau$ to the robot time axis $t$. We make the following reasonable assumption:  $w$ is strictly increasing for the range of $[0,T]$ and continuously differentiable function with $w(0)=0$.

Given the sparse demonstrations $\mathcal{D}$, **the problem of interest** is  to find an objective function parameter $\boldsymbol{p}$ and a time-warping function $w$ such that the following trajectory loss is minimized:


$$
\min\limits_{\boldsymbol{p},w}\sum\nolimits_{i=1}^{N}l\Big(\boldsymbol{y}^*(\tau_i),
\boldsymbol{g}\big(\boldsymbol{\xi}_{\boldsymbol{p}}(w(\tau_{i}))   \big)    \Big),
\label{loss}
$$

where ${l}(\boldsymbol{a},\boldsymbol{b})$ is  a given differentiable scalar function to quantify a point distance metric between vectors $\boldsymbol{a}$ and $\boldsymbol{b}$, e.g., $l(\boldsymbol{a},\boldsymbol{b})=||\boldsymbol{a}-\boldsymbol{b}||^2$. 
Minimizing the loss in (\ref{loss}) means that we want  the robot to find the `best' objective function within the parameterized objective function set (\ref{costfun}), together with a time-warping function, such that its reproduced trajectory is as close to the given sparse demonstrations as possible.



<p style="margin-bottom:2cm; margin-left: 0.5cm"> </p>

-----
# <center> Proposed Method </center>



<center>Please read the paper for the proposed method.</center>
-----

<p style="margin-bottom:2cm; margin-left: 0.5cm"> </p>




## Inverted Pendulum Example


We apply the proposed method to learn the control objective function for an inverted pendulum control system.  The expert only provides sparse position waypoints of the inverted pendulum. The objective function is parametrized as a weighted distance to a goal state or as a neural network, and the time-warping function is parameterized using polynomial functions.  More details and experiments can be found in the paper.

* Realizable case: the given sparse demonstrations correspond to an exact objective function and time-warping function, from which the robot's reproduced trajectory can exactly passes through these sparse demonstrations.

  <center>
  <img src="/images/pen_loss.png" alt="drawing" width="600" align="center"/>
  </center>
  <p style="margin-top:0.5cm;font-size:15px"> Results for the realizable case. Left: the loss value  (\ref{loss}) versus the number of iterations. Right: the convergence of the pendulum's (time-warped) trajectory as iteration increases, where the color from light to dark  gray  corresponds to increasing  iteration number, and the red dots are the sparse demonstration waypoints (only position information).  </p>



* Non-realizable case: the sparse demonstrations are given randomly, and there might not exist an exact objective function or time-warping function whose optimal trajectory exactly passes through the sparse demonstrations.

  <center>
  <img src="/images/pen_loss2.png" alt="drawing" width="600" align="center"/>
  </center>
  <p style="margin-top:0.5cm;font-size:15px"> Reults for the non-realizable case. Left: the loss value  (\ref{loss}) versus the number of iterations. Right: the convergence of the pendulum's (time-warped) trajectory as iteration increases, where the color from light to dark  gray  corresponds to increasing  iteration number, and the red dots are the  sparse demonstration waypoint randomly given (only position information).  </p>



* Neural objective function: the sparse demonstrations are given randomly (as in the non-realizable case), and here we use a neural network to represent the robotâ€™s objective function. We aim to learn a neural objective function and a time-warping function.

  <center>
  <img src="/images/pen_trajectory_nnetwork2.png" alt="drawing" width="600" align="center"/>
  </center>
  <p style="margin-top:0.5cm;font-size:15px"> Learning from sparse waypoints with the  objective function represented by a neural network. Left: the loss value  (\ref{loss}) versus the number of iterations. Right: the  (time-warped) trajectory produced by the learned neural objective function, where  the red dots are the given sparse  waypoints (the same as in the non-realizable case). Clearly, under the same sparse demonstrations, using a neural objective functions can achieve  a lower loss than using  weighted-distance parameterization of the objective function  in the non-realizable case.   </p>




## Application: Learning for Obstacle Avoidance
We apply the proposed method to learn robot motion control/planning   in  environments with obstacles. Here, a human demonstrator  provides few **position** waypoints in the vicinity of obstacles, and a robot learns a control objective function from those waypoints such that its resulting motion can follow these demonstrated waypoints and get around the obstacles. We demonstrate this on two systems: a 6-DoF maneuvering UAV and a two-link robot arm. More details and experiments can be found in the paper.
 
### 6-DoF maneuvering UAV
 
<center>
<img src="/images/6dof-uav-lfsd.gif" alt="drawing" width="600" align="center"/>
</center>
 <p style="margin-top:0.5cm;font-size:15px;margin-left: 0.5cm"> We gradually increase the number of demonstrated waypoints from one to five while observing if the UAV, after learning an control objective function from the sparse waypoints, can successfully fly through the two gates without collision in each case. The results show that by demonstrating only five position waypoints, the UAV is able to learn a valid objective function to successfuly fly through the two gates and land on the target position. Also find this video on <a href="https://www.youtube.com/watch?v=awVNiCIJCfs" >[<img src="/images/youtube.png" width="25px" height="25px">]</a>
</p>

### Two-link robot arm reaching motion

<center>
<img src="/images/robot-arm-lfsd.gif" alt="drawing" width="600" align="center"/>
</center>
 <p style="margin-top:0.5cm;font-size:15px;margin-left: 0.5cm"> In order to guide the robot arm to avoid the obstacle while reaching the goal configuration, we demonstrate one position waypoint for the robot arm. With this single demonstrated waypoint, the robot arm learns a valid objective function to successfully avoid obstacles and reach the target point. Also find this video on <a href="https://www.youtube.com/watch?v=awVNiCIJCfs" >[<img src="/images/youtube.png" width="25px" height="25px">]</a></p>

