---
title: 'Safe Pontryagin Differentiable Programming'
date: 2021-06-01
permalink: /posts/safePDP
tags:
  - PDP
  - safety-critical
  - safe learning
  - safe control
---




<p style="margin-bottom:1cm; margin-left: 0.5cm"> </p>



<p style="margin-top:0.8cm;">  </p>
-----
Paper: [Safe Pontryagin Differentiable Programming](https://arxiv.org/abs/2105.14937) <br />
Author: [Wanxin Jin](https://wanxinjin.github.io/), 
[Shaoshuai Mou](https://engineering.purdue.edu/AAE/people/ptProfile?resource_id=124981), and [George J. Pappas](https://www.georgejpappas.org/) <br />
Codes: [https://github.com/wanxinjin/Safe-PDP](https://github.com/wanxinjin/Safe-PDP)

-----

<br />


<p style="margin-bottom:1cm; margin-left: 0.5cm"> </p>

# <center> Motivation <center>
Safety is usually a  priority in the deployment of a learning and control algorithm to real-world systems. For a physical system (agent), safety is normally given in various states and inputs constraints, which must not be violated by the algorithm at _any stage_ of the learning and control process, otherwise will cause irrevocable or unacceptable failure/damage/loss. Those systems are  referred to as _safety-critical_. The constraints in  a safety-critical system can  include the _immediate_ ones, which are directly  imposed on  the system state and input  at certain/all time instances, and the _long-term_ ones, which are defined on the   trajectory of system states and inputs over a long time period. 



Compared to the abundant results that  focus on   system optimality,   systematic and principled treatments for safety-critical learning and control problems  seem largely insufficient, particularly in the following  gaps (see the related work section in the paper. First, existing safety strategies are either too conservative, which may restrict the  task performance, or violation-tolerable, which only pursues the near-constraint guarantee and thus are not strictly  constraint-respecting. Second, a generic safety paradigm capable  of handling different types of constraints, including system state and input (or mixed), immediate, or/and long-term constraints, is still lacked. Third,  some  existing safety strategies suffer from huge computational- and data- complexity,  difficult to be integrated into any differentiable programming frameworks to solve  large-scale  learning and continuous control tasks.




To address the above research gaps, this paper aims to develop a new theoretical and algorithmic framework for safe learning and control with the following  key  abilities.  

- First, the framework provides a systematic treatment for **different types of  constraints** in a safety-critical problem;
- second, it attains **provable safety- and optimality- guarantees** throughout the  learning and control process;
- third, it is flexible   to perform **safe learning of any unknown aspects of a safety-critical system**, including  policy, dynamics, state and input constraints, and control cost function;
- finally, it can be integrated into any **differentiable programming framework** to efficiently solve large-scale safe learning and control tasks.

-----

<br />
<br />


# <center> Contributions of Safe PDP <center>
This [paper](https://arxiv.org/abs/2105.14937) proposes a safe differentiable programming methodology named as _Safe Pontryagin Differentiable Programming (or Safe PDP)_. Safe PDP provides a systematic treatment of different types of system constraints, including  state and inputs (or mixed), immediate, and long-term constraints, with provable safety- and performance-guarantee. Safe PDP also establishes a unified differentiable programming framework to efficiently solve a board class of safety-critical learning and control tasks, such as safe policy optimization, safe motion  planning, and safe learning from demonstrations, etc. 
 
In the spirit of  interior-point methods, Safe PDP incorporates different types of  constraints into the  control cost and loss through  barrier functions, approximating a constrained  problem using an unconstrained counterpart. The contributions of  Safe PDP are both theoretical and algorithmic. Theoretically, we prove that  (I)  not only the constrained solution but also its gradient in backward pass can be safely approximated by solving a more efficient unconstrained counterpart (see Theorem 2);  (II) any intermediate results throughout the approximation and optimization are strictly safety-guaranteed, i.e., never violating  constraints, (see Theorem 2 and Theorem 3); and (III)  the approximations for both solution and gradient can be controlled for arbitrarily accuracy by a simple barrier parameter (see Theorem 2 and Theorem 3),  which could mean that the conservativeness (i.e., over-approximations) for safety can be well controlled. Arithmetically, (IV) Safe PDP also provides an efficient algorithm to differentiate the solution of a constrained optimal control system (see Theorem 1); and (V)  Safe PDP is capable and versatile  to efficiently solve a board class of safety-critical learning and control problems, including  safe policy optimization, safe motion planning,   learning MPCs from demonstrations (see Section 7). All the above theoretical and  algorithmic features of   Safe PDP has not been  (fully) achieved in any previous work  to authors' best knowledge.

-----

<br />
<br />



# <center> Representative Applications of Safe PDP<center>

## Safe neural policy optimization

<center>
<p style="margin-top:0.8cm;">  </p>
<a href="https://youtu.be/sC81qc2ip8U">
<img src="/images/SafePDP_SPO.png" alt="drawing" width="600" align="center"/>
</a>
<p style="margin-top:0.1cm;"> Click to play  </p>
</center>







## Safe motion planning

<center>
<p style="margin-top:0.8cm;">  </p>
<a href="https://youtu.be/vZVxgo30mDs">
<img src="/images/SafePDP_SPlan.png" alt="drawing" width="600" align="center"/>
</a>
<p style="margin-top:0.1cm;"> Click to play  </p>
</center>






## Learning MPC from demonstrations


<center>
<p style="margin-top:0.8cm;">  </p>
<a href="https://youtu.be/OBiLYYlWi98">
<img src="/images/SafePDP_MPC.png" alt="drawing" width="600" align="center"/>
</a>
<p style="margin-top:0.1cm;"> Click to play  </p>
</center>

<br />
<br />

